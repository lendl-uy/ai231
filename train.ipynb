{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from constants import *\n",
    "from read_dataset import *\n",
    "from visualization import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(tokenizer, doc_texts, doc_labels, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    label_ids = []\n",
    "\n",
    "    # Define a mapping of labels to integer IDs\n",
    "    label_map = {\n",
    "        'O': 0,\n",
    "        'B-STREET_ADDRESS': 1, 'I-STREET_ADDRESS': 2,\n",
    "        'B-PHONE_NUM': 3, 'I-PHONE_NUM': 4,\n",
    "        'B-URL_PERSONAL': 5, 'I-URL_PERSONAL': 6,\n",
    "        'B-ID_NUM': 7, 'I-ID_NUM': 8,\n",
    "        'B-NAME_STUDENT': 9, 'I-NAME_STUDENT': 10,\n",
    "        'B-USERNAME': 11,\n",
    "        'B-EMAIL': 12\n",
    "    }\n",
    "\n",
    "    for doc_text, doc_label in tqdm(zip(doc_texts, doc_labels), total=len(doc_texts), desc=\"Encoding documents\"):\n",
    "        for text, labels in zip(doc_text, doc_label):\n",
    "            # Ensure text is a list of words\n",
    "            if isinstance(text, str):\n",
    "                text = text.split()  # This line might not be necessary if text is already a list of words\n",
    "\n",
    "            tokenized_text = tokenizer.encode_plus(\n",
    "                text,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=True,\n",
    "                return_offsets_mapping=True,\n",
    "                is_split_into_words=True\n",
    "            )\n",
    "            offsets = tokenized_text.pop(\"offset_mapping\")\n",
    "            input_ids.append(tokenized_text['input_ids'])\n",
    "            attention_masks.append(tokenized_text['attention_mask'])\n",
    "            token_type_ids.append(tokenized_text['token_type_ids'])\n",
    "\n",
    "            label_sequence = [label_map[\"O\"]] * max_length\n",
    "            current_label_index = 0\n",
    "\n",
    "            for offset in offsets:\n",
    "                if offset[0] == offset[1]:  # Special tokens like [CLS], [SEP], padding\n",
    "                    continue\n",
    "                if current_label_index < len(labels):\n",
    "                    label_sequence[offset[0]] = label_map.get(labels[current_label_index], label_map[\"O\"])\n",
    "                    current_label_index += 1\n",
    "            label_ids.append(label_sequence)\n",
    "\n",
    "    return {\n",
    "        'input_ids': tf.constant(input_ids),\n",
    "        'attention_mask': tf.constant(attention_masks),\n",
    "        'token_type_ids': tf.constant(token_type_ids),\n",
    "    }, tf.constant(label_ids)\n",
    "        \n",
    "def predict(tokenizer, model, text):\n",
    "    tokenized_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    predictions = model(tokenized_text)\n",
    "    logits = predictions.logits\n",
    "    predicted_label_ids = tf.argmax(logits, axis=-1)\n",
    "    return predicted_label_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since train.json is too large, it was zipped\n",
    "# To read the file, unzip then pass to the json parser   \n",
    "unzip_file(ZIPPED_TRAIN_SET_PATH, \"datasets/\")\n",
    "    \n",
    "document_numbers_train, texts_train, tokens_train, trailing_whitespaces_train, labels_train = read_pii_json(TRAIN_SET_PATH, is_train=True)\n",
    "document_numbers_test, texts_test, tokens_test, trailing_whitespaces_test = read_pii_json(TEST_SET_PATH)\n",
    "\n",
    "flat_labels = [label for sublist in labels_train for label in sublist]\n",
    "unique_labels = set(flat_labels)\n",
    "\n",
    "# Count the label frequencies\n",
    "label_counts = Counter(flat_labels)\n",
    "\n",
    "# Prepare data for plotting\n",
    "labels, frequencies = zip(*label_counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F5Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f5_score\", **kwargs):\n",
    "        super(F5Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "        self.beta_squared = 5**2\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return (1 + self.beta_squared) * ((precision * recall) / ((self.beta_squared * precision) + recall))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding documents: 100%|██████████| 6807/6807 [06:41<00:00, 16.94it/s]  \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(encode_texts(tokenizer, tokens_train, labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute '_distribute_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m metric \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m512\u001b[39m), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# # Example prediction\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# # predicted_label_ids = predict(\"Your example sentence here.\", model=model, tokenizer=tokenizer)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lendl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1504\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[1;32m-> 1504\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1505\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   1506\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   1507\u001b[0m         metrics\u001b[38;5;241m=\u001b[39mmetrics,\n\u001b[0;32m   1508\u001b[0m         loss_weights\u001b[38;5;241m=\u001b[39mloss_weights,\n\u001b[0;32m   1509\u001b[0m         weighted_metrics\u001b[38;5;241m=\u001b[39mweighted_metrics,\n\u001b[0;32m   1510\u001b[0m         run_eagerly\u001b[38;5;241m=\u001b[39mrun_eagerly,\n\u001b[0;32m   1511\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39msteps_per_execution,\n\u001b[0;32m   1512\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1513\u001b[0m     )\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1515\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1516\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   1517\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1524\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lendl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\lendl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4021\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended.variable_created_in_scope\u001b[1;34m(self, v)\u001b[0m\n\u001b[0;32m   4020\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvariable_created_in_scope\u001b[39m(\u001b[38;5;28mself\u001b[39m, v):\n\u001b[1;32m-> 4021\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribute_strategy\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Variable' object has no attribute '_distribute_strategy'"
     ]
    }
   ],
   "source": [
    "num_labels = len(unique_labels)  # Define the number of unique labels\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=num_labels)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.fit(train_dataset.shuffle(1000).batch(512), epochs=3, batch_size=16)\n",
    "\n",
    "# # Example prediction\n",
    "# # predicted_label_ids = predict(\"Your example sentence here.\", model=model, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
