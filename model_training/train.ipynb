{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation of packages\n",
    "\n",
    "Script to install packages for easier dependency tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install tensorflow==2.16.1 transformers tqdm numpy matplotlib ipykernel seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lendl\\Documents\\ai231\\staging\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import tf_keras\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from training_constants import *\n",
    "from read_dataset import *\n",
    "from visualization import visualize_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(tokenizer, doc_texts, doc_labels, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    label_ids = []\n",
    "\n",
    "    # Define a mapping of labels to integer IDs\n",
    "    label_map = {\n",
    "        'O': 0,\n",
    "        'B-STREET_ADDRESS': 1, 'I-STREET_ADDRESS': 2,\n",
    "        'B-PHONE_NUM': 3, 'I-PHONE_NUM': 4,\n",
    "        'B-URL_PERSONAL': 5, 'I-URL_PERSONAL': 6,\n",
    "        'B-ID_NUM': 7, 'I-ID_NUM': 8,\n",
    "        'B-NAME_STUDENT': 9, 'I-NAME_STUDENT': 10,\n",
    "        'B-USERNAME': 11,\n",
    "        'B-EMAIL': 12\n",
    "    }\n",
    "\n",
    "    for doc_text, doc_label in tqdm(zip(doc_texts, doc_labels), total=len(doc_texts), desc=\"Encoding documents\"):\n",
    "        for text, labels in zip(doc_text, doc_label):\n",
    "            # Ensure text is a list of words\n",
    "            if isinstance(text, str):\n",
    "                text = text.split()  # This line might not be necessary if text is already a list of words\n",
    "\n",
    "            tokenized_text = tokenizer.encode_plus(\n",
    "                text,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=True,\n",
    "                return_offsets_mapping=True,\n",
    "                is_split_into_words=True\n",
    "            )\n",
    "            offsets = tokenized_text.pop(\"offset_mapping\")\n",
    "            input_ids.append(tokenized_text['input_ids'])\n",
    "            attention_masks.append(tokenized_text['attention_mask'])\n",
    "            token_type_ids.append(tokenized_text['token_type_ids'])\n",
    "\n",
    "            label_sequence = [label_map[\"O\"]] * max_length\n",
    "            current_label_index = 0\n",
    "\n",
    "            for offset in offsets:\n",
    "                if offset[0] == offset[1]:  # Special tokens like [CLS], [SEP], padding\n",
    "                    continue\n",
    "                if current_label_index < len(labels):\n",
    "                    label_sequence[offset[0]] = label_map.get(labels[current_label_index], label_map[\"O\"])\n",
    "                    current_label_index += 1\n",
    "            label_ids.append(label_sequence)\n",
    "\n",
    "    return {\n",
    "        'input_ids': tf.constant(input_ids),\n",
    "        'attention_mask': tf.constant(attention_masks),\n",
    "        'token_type_ids': tf.constant(token_type_ids),\n",
    "    }, tf.constant(label_ids)\n",
    "        \n",
    "def predict(tokenizer, model, text):\n",
    "    tokenized_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    predictions = model(tokenized_text)\n",
    "    logits = predictions.logits\n",
    "    predicted_label_ids = tf.argmax(logits, axis=-1)\n",
    "    return predicted_label_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since train.json is too large, it was zipped\n",
    "# To read the file, unzip then pass to the json parser   \n",
    "unzip_file(ZIPPED_TRAIN_SET_PATH, \"../datasets/\")\n",
    "    \n",
    "document_numbers_train, texts_train, tokens_train, trailing_whitespaces_train, labels_train = read_pii_json(TRAIN_SET_PATH, is_train=True)\n",
    "document_numbers_test, texts_test, tokens_test, trailing_whitespaces_test = read_pii_json(TEST_SET_PATH)\n",
    "\n",
    "# flat_labels = [label for sublist in labels_train for label in sublist]\n",
    "# unique_labels = set(flat_labels)\n",
    "\n",
    "# # Count the label frequencies\n",
    "# label_counts = Counter(flat_labels)\n",
    "\n",
    "# # Prepare data for plotting\n",
    "# labels, frequencies = zip(*label_counts.items())\n",
    "\n",
    "# visualize_labels(labels, frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F5Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f5_score\", **kwargs):\n",
    "        super(F5Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "        self.beta_squared = 5**2\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return (1 + self.beta_squared) * ((precision * recall) / ((self.beta_squared * precision) + recall))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding documents: 100%|██████████| 6807/6807 [07:35<00:00, 14.96it/s]  "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoded_texts = encode_texts(tokenizer, tokens_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# encoded_texts_in_np = np.array(encoded_texts)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(encoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute '_distribute_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m16\u001b[39m), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lendl\\Documents\\ai231\\staging\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1504\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[1;32m-> 1504\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1505\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   1506\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   1507\u001b[0m         metrics\u001b[38;5;241m=\u001b[39mmetrics,\n\u001b[0;32m   1508\u001b[0m         loss_weights\u001b[38;5;241m=\u001b[39mloss_weights,\n\u001b[0;32m   1509\u001b[0m         weighted_metrics\u001b[38;5;241m=\u001b[39mweighted_metrics,\n\u001b[0;32m   1510\u001b[0m         run_eagerly\u001b[38;5;241m=\u001b[39mrun_eagerly,\n\u001b[0;32m   1511\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39msteps_per_execution,\n\u001b[0;32m   1512\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1513\u001b[0m     )\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1515\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1516\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   1517\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1524\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lendl\\Documents\\ai231\\staging\\lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\lendl\\Documents\\ai231\\staging\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4021\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended.variable_created_in_scope\u001b[1;34m(self, v)\u001b[0m\n\u001b[0;32m   4020\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvariable_created_in_scope\u001b[39m(\u001b[38;5;28mself\u001b[39m, v):\n\u001b[1;32m-> 4021\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribute_strategy\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Variable' object has no attribute '_distribute_strategy'"
     ]
    }
   ],
   "source": [
    "# Define the number of unique labels for the classification task\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "# Initialize the model for token classification with BERT tiny\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=num_labels)\n",
    "\n",
    "# Define the optimizer, loss, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "\n",
    "# Compile the model\n",
    "# model._distribute_strategy = None\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
