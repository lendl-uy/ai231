{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation of packages\n",
    "\n",
    "Script to install packages for easier dependency tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_nlp\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from training_constants import *\n",
    "from read_dataset import *\n",
    "from ..visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(tokenizer, doc_texts, doc_labels, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    label_ids = []\n",
    "\n",
    "    # Define a mapping of labels to integer IDs\n",
    "    label_map = {\n",
    "        'O': 0,\n",
    "        'B-STREET_ADDRESS': 1, 'I-STREET_ADDRESS': 2,\n",
    "        'B-PHONE_NUM': 3, 'I-PHONE_NUM': 4,\n",
    "        'B-URL_PERSONAL': 5, 'I-URL_PERSONAL': 6,\n",
    "        'B-ID_NUM': 7, 'I-ID_NUM': 8,\n",
    "        'B-NAME_STUDENT': 9, 'I-NAME_STUDENT': 10,\n",
    "        'B-USERNAME': 11,\n",
    "        'B-EMAIL': 12\n",
    "    }\n",
    "\n",
    "    for doc_text, doc_label in tqdm(zip(doc_texts, doc_labels), total=len(doc_texts), desc=\"Encoding documents\"):\n",
    "        for text, labels in zip(doc_text, doc_label):\n",
    "            # Ensure text is a list of words\n",
    "            if isinstance(text, str):\n",
    "                text = text.split()  # This line might not be necessary if text is already a list of words\n",
    "\n",
    "            tokenized_text = tokenizer.encode_plus(\n",
    "                text,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=True,\n",
    "                return_offsets_mapping=True,\n",
    "                is_split_into_words=True\n",
    "            )\n",
    "            offsets = tokenized_text.pop(\"offset_mapping\")\n",
    "            input_ids.append(tokenized_text['input_ids'])\n",
    "            attention_masks.append(tokenized_text['attention_mask'])\n",
    "            token_type_ids.append(tokenized_text['token_type_ids'])\n",
    "\n",
    "            label_sequence = [label_map[\"O\"]] * max_length\n",
    "            current_label_index = 0\n",
    "\n",
    "            for offset in offsets:\n",
    "                if offset[0] == offset[1]:  # Special tokens like [CLS], [SEP], padding\n",
    "                    continue\n",
    "                if current_label_index < len(labels):\n",
    "                    label_sequence[offset[0]] = label_map.get(labels[current_label_index], label_map[\"O\"])\n",
    "                    current_label_index += 1\n",
    "            label_ids.append(label_sequence)\n",
    "\n",
    "    return {\n",
    "        'input_ids': tf.constant(input_ids),\n",
    "        'attention_mask': tf.constant(attention_masks),\n",
    "        'token_type_ids': tf.constant(token_type_ids),\n",
    "    }, tf.constant(label_ids)\n",
    "        \n",
    "def predict(tokenizer, model, text):\n",
    "    tokenized_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    predictions = model(tokenized_text)\n",
    "    logits = predictions.logits\n",
    "    predicted_label_ids = tf.argmax(logits, axis=-1)\n",
    "    return predicted_label_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since train.json is too large, it was zipped\n",
    "# To read the file, unzip then pass to the json parser   \n",
    "unzip_file(ZIPPED_TRAIN_SET_PATH, \"datasets/\")\n",
    "    \n",
    "document_numbers_train, texts_train, tokens_train, trailing_whitespaces_train, labels_train = read_pii_json(TRAIN_SET_PATH, is_train=True)\n",
    "document_numbers_test, texts_test, tokens_test, trailing_whitespaces_test = read_pii_json(TEST_SET_PATH)\n",
    "\n",
    "flat_labels = [label for sublist in labels_train for label in sublist]\n",
    "unique_labels = set(flat_labels)\n",
    "\n",
    "# Count the label frequencies\n",
    "label_counts = Counter(flat_labels)\n",
    "\n",
    "# Prepare data for plotting\n",
    "labels, frequencies = zip(*label_counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F5Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f5_score\", **kwargs):\n",
    "        super(F5Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "        self.beta_squared = 5**2\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return (1 + self.beta_squared) * ((precision * recall) / ((self.beta_squared * precision) + recall))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoded_texts = encode_texts(tokenizer, tokens_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "encoded_texts_in_np = np.array(encoded_texts)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(encoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of unique labels for the classification task\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "# Initialize the model for token classification with BERT tiny\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=num_labels)\n",
    "\n",
    "# Define the optimizer, loss, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
