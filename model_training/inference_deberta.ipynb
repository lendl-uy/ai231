{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "sample_text = \"\"\"John Doe was born on January 1, 1990, in New York City. He has lived in NYC all his life and graduated from Columbia University in 2012 with a degree in Computer Science. John's email address is johndoe@example.com, and his phone number is (555) 123-4567. Recently, he moved to a new house located at 123 Main St, New York, NY 10001. John works at TechSolutions, where he is a senior software engineer. His Social Security number is 123-45-6789.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lendl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# Paths and configurations\n",
    "DEBERTA_MODEL_PATH = \"models/deberta3base_1024\"\n",
    "INFERENCE_MAX_LENGTH = 2048\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEBERTA_MODEL_PATH)\n",
    "model = AutoModelForTokenClassification.from_pretrained(DEBERTA_MODEL_PATH)\n",
    "\n",
    "# Load id2label mapping\n",
    "config = json.load(open(Path(DEBERTA_MODEL_PATH) / \"config.json\"))\n",
    "id2label = config[\"id2label\"]\n",
    "\n",
    "# Tokenize the sample text\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, max_length=INFERENCE_MAX_LENGTH)\n",
    "\n",
    "# Retrieve tokens from IDs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "# Prepare the data collator\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "# Prepare the inputs for the trainer\n",
    "dataset = Dataset.from_dict(inputs)\n",
    "dataset.set_format(\"torch\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, collate_fn=collator)\n",
    "\n",
    "# Prediction setup\n",
    "args = TrainingArguments(\".\", per_device_eval_batch_size=1, report_to=\"none\")\n",
    "trainer = Trainer(model=model, args=args, data_collator=collator, tokenizer=tokenizer)\n",
    "\n",
    "# Predict and convert logits to labels\n",
    "for batch in dataloader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "\n",
    "    # Convert predictions to readable labels using id2label\n",
    "    pred_labels = [id2label[str(int(index))] for index in predictions[0].flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text(tokens, labels):\n",
    "    # Initialize an empty string for the reconstructed text\n",
    "    reconstructed_text = \"\"\n",
    "    # Loop through each token and corresponding label\n",
    "    for token, label in zip(tokens, labels):\n",
    "        # Skip special tokens\n",
    "        if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "            continue\n",
    "        # Remove the first underscore and any subsequent underscores (subword pieces)\n",
    "        if token.startswith(\"‚ñÅ\"):\n",
    "            # Add a space before starting a new word (if not the start of the string)\n",
    "            if reconstructed_text:\n",
    "                reconstructed_text += \" \"\n",
    "            # Add the cleaned token (without the underscore)\n",
    "            reconstructed_text += token[1:]\n",
    "        else:\n",
    "            # Directly append subword pieces to the last word (no space)\n",
    "            reconstructed_text += token\n",
    "    \n",
    "    return reconstructed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"John Doe was born on January 1, 1990, in New York City. He has lived in NYC all his life and graduated from Columbia University in 2012 with a degree in Computer Science. John's email address is johndoe@example.com, and his phone number is (555) 123-4567. Recently, he moved to a new house located at 123 Main St, New York, NY 10001. John works at TechSolutions, where he is a senior software engineer. His Social Security number is 123-45-6789.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruct_text(tokens, pred_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
